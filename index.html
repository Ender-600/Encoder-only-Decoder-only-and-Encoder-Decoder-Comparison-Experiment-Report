<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="E¬≥-Mini Benchmark: The End Game of Architecture Wars?">
  <meta name="description" content="A comprehensive evaluation of Efficiency, Energy, and Effectiveness across Encoder-only, Decoder-only, and Encoder-Decoder architectures. Quantifying the Generality Tax in modern LLMs.">
  <meta name="keywords" content="transformer architectures, encoder-decoder, energy efficiency, BERT, GPT-2, neural architecture comparison, NLP benchmarking, LLM efficiency, machine learning, deep learning">
  <meta name="author" content="E¬≥ Benchmark Research Team">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="E¬≥ Mini-Benchmark Research">
  <meta property="og:title" content="E¬≥-Mini Benchmark: The End Game of Architecture Wars?">
  <meta property="og:description" content="A comprehensive evaluation of Efficiency, Energy, and Effectiveness across Encoder-only, Decoder-only, and Encoder-Decoder architectures. Quantifying the Generality Tax in modern LLMs.">
  <meta property="og:url" content="https://your-domain.com/e3-benchmark">
  <meta property="og:image" content="https://your-domain.com/static/images/pareto_frontier.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="E¬≥ Mini-Benchmark - Pareto Frontier Analysis">
  <meta property="article:published_time" content="2025-01-01T00:00:00.000Z">
  <meta property="article:author" content="E¬≥ Research Team">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="transformer architectures">
  <meta property="article:tag" content="energy efficiency">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@E3Benchmark">
  <meta name="twitter:creator" content="@E3Benchmark">
  <meta name="twitter:title" content="E¬≥-Mini Benchmark: The End Game of Architecture Wars?">
  <meta name="twitter:description" content="A comprehensive evaluation of Efficiency, Energy, and Effectiveness across Encoder-only, Decoder-only, and Encoder-Decoder architectures. Quantifying the Generality Tax in modern LLMs.">
  <meta name="twitter:image" content="https://your-domain.com/static/images/pareto_frontier.png">
  <meta name="twitter:image:alt" content="E¬≥ Mini-Benchmark - Pareto Frontier Analysis">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="E¬≥-Mini Benchmark: A Full-Dimensional Evaluation of Efficiency, Energy, and Effectiveness in Transformer Architectures">
  <meta name="citation_author" content="Research Team, E¬≥">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Machine Learning Research">
  <meta name="citation_pdf_url" content="https://your-domain.com/static/pdfs/e3_benchmark_paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>The End Game of Architecture Wars? | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "The End Game of Architecture Wars?",
    "description": "A comprehensive evaluation of Efficiency, Energy, and Effectiveness across Encoder-only, Decoder-only, and Encoder-Decoder architectures",
    "author": [
      {
        "@type": "Person",
        "name": "Boyu Liu",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Illinois at Urbana-Champaign"
        }
      }
    ],
    "datePublished": "2025-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "Machine Learning Research"
    },
    "url": "https://your-domain.com/e3-benchmark",
    "image": "https://your-domain.com/static/images/pareto_frontier.png",
    "keywords": ["transformer architectures", "encoder-decoder", "energy efficiency", "BERT", "GPT-2", "neural architecture comparison"],
    "abstract": "While Decoder-only architectures rule the LLM era, our E¬≥ evaluation reveals they suffer from massive inefficiencies in specific scenarios. We quantify the 'Generality Tax' in energy, the 'KV-Cache Bottleneck' in long-context inference, and the 'Sample Inefficiency' in data-scarce regimes.",
    "citation": "@article{E3MiniBenchmark2025, title={Experimental Report on the Comparison of Encoder-only, Decoder-only, and Encoder-Decoder Architectures}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://your-domain.com/e3-benchmark"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Neural Architecture Comparison"
      },
      {
        "@type": "Thing", 
        "name": "Energy Efficiency in AI"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "E¬≥ Mini-Benchmark Research",
    "url": "https://your-domain.com",
    "logo": "https://your-domain.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/E3Benchmark",
      "https://github.com/your-username/e3-benchmark"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown - Commented out for this project -->
  <!-- 
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="#" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Related Work 1</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>
  -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"> The End Game of Architecture Wars?</h1>
            <h2 class="subtitle is-4 publication-subtitle" style="margin-top: 1rem; color: #4a5568;">
              Experimental Report on the Comparison of Encoder-only, Decoder-only, and Encoder-Decoder Architectures
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#" target="_blank">Boyu Liu</a><sup>1</sup>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Illinois at Urbana-Champaign<br>2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0012829800004547" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <span class="link-block">
                    <a href="https://github.com/Ender-600/E3-mini-benchmark" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- <span class="link-block">
                  <a href="https://huggingface.co/datasets/your-org/e3-benchmark-data" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser Figure: Key Visual -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/pareto_frontier.png" alt="Pareto Frontier: Accuracy vs Energy Consumption" style="width: 100%; max-width: 900px; margin: 0 auto; display: block; border-radius: 8px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);" loading="eager"/>
      <h2 class="subtitle has-text-centered" style="margin-top: 1.5rem; font-size: 1.1rem; line-height: 1.6;">
        <strong>The Generality Tax:</strong> Decoder-only models (Red) achieve high generality but pay a <strong>5√ó‚Äì10√ó energy premium</strong> compared to Encoder-only models (Blue) on Natural Language Understanding tasks.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser figure -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified" style="font-size: 1.05rem; line-height: 1.8;">
          <p>
            While <strong>Decoder-only architectures</strong> dominate the Large Language Model (LLM) era with their unprecedented scaling capabilities, our comprehensive <strong>E¬≥ (Efficiency, Energy, Effectiveness) evaluation</strong> reveals that they suffer from massive inefficiencies in specific deployment scenarios. Through systematic benchmarking across three transformer paradigms‚Äî<strong>Encoder-only (BERT)</strong>, <strong>Decoder-only (GPT-2)</strong>, and <strong>Encoder-Decoder (T5)</strong>‚Äîwe quantify three critical phenomena:
          </p>
          <p style="margin-top: 1rem;">
            <strong>(1) The Generality Tax:</strong> Decoder-only models consume 5√ó‚Äì10√ó more energy per prediction on classification tasks compared to specialized encoder architectures, despite achieving comparable accuracy.
          </p>
          <p style="margin-top: 0.5rem;">
            <strong>(2) The KV-Cache Bottleneck:</strong> In long-context inference (>2K tokens), decoder-only models experience exponential VRAM growth, hitting out-of-memory errors at 4K tokens on V100 GPUs, while encoder models scale linearly.
          </p>
          <p style="margin-top: 0.5rem;">
            <strong>(3) Sample Inefficiency:</strong> In data-scarce regimes (<1000 samples), encoder inductive biases lead to 2√ó faster convergence during domain adaptation compared to generative pretraining.
          </p>
          <p style="margin-top: 1rem;">
            Our findings challenge the "one-size-fits-all" narrative in neural architecture selection and provide actionable insights for sustainable AI system design.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Methodology: The E¬≥ Framework -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered" style="margin-bottom: 2rem;">The E¬≥ Evaluation Framework</h2>
      <div class="content" style="font-size: 1rem; line-height: 1.7;">
        <p class="has-text-centered" style="margin-bottom: 2.5rem; font-size: 1.1rem; color: #4a5568;">
          We propose a <strong>three-dimensional assessment</strong> protocol that goes beyond traditional accuracy metrics to capture the full lifecycle cost of neural architectures:
        </p>
        
        <div class="columns is-multiline" style="margin-top: 2rem;">
          <div class="column is-one-third">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 12px; padding: 2rem;">
              <h3 class="title is-4" style="color: white; margin-bottom: 1rem;">
                <span class="icon" style="font-size: 2rem; display: block; margin-bottom: 0.5rem;">‚ö°</span>
                Efficiency
              </h3>
              <p style="font-size: 0.95rem; line-height: 1.6;">
                <strong>Throughput:</strong> Tokens processed per second<br>
                <strong>Latency:</strong> Time to first token & end-to-end inference<br>
                <strong>Context Scaling:</strong> Performance degradation with sequence length
              </p>
            </div>
          </div>
          
          <div class="column is-one-third">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; border-radius: 12px; padding: 2rem;">
              <h3 class="title is-4" style="color: white; margin-bottom: 1rem;">
                <span class="icon" style="font-size: 2rem; display: block; margin-bottom: 0.5rem;">üîã</span>
                Energy
              </h3>
              <p style="font-size: 0.95rem; line-height: 1.6;">
                <strong>Power Draw:</strong> Instantaneous GPU wattage (Watts)<br>
                <strong>Total Energy:</strong> Cumulative consumption per task (kWh)<br>
                <strong>Carbon Footprint:</strong> CO‚ÇÇ equivalent emissions
              </p>
            </div>
          </div>
          
          <div class="column is-one-third">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; border-radius: 12px; padding: 2rem;">
              <h3 class="title is-4" style="color: white; margin-bottom: 1rem;">
                <span class="icon" style="font-size: 2rem; display: block; margin-bottom: 0.5rem;">üéØ</span>
                Effectiveness
              </h3>
              <p style="font-size: 0.95rem; line-height: 1.6;">
                <strong>Task Performance:</strong> SuperGLUE accuracy & F1 scores<br>
                <strong>Convergence Speed:</strong> Training steps to target performance<br>
                <strong>Generalization:</strong> Cross-domain transfer capability
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Methodology -->




<!-- Experimental Results -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered" style="margin-bottom: 2.5rem;">Experimental Results</h2>
      
      <!-- Experiment 1: The Cost of Generality -->
      <div class="box" style="margin-bottom: 3rem; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.08);">
        <h3 class="title is-4" style="color: #2d3748; margin-bottom: 1.5rem;">
          <span style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text;">
            Experiment 1: The Cost of Generality
          </span>
        </h3>
        <div class="columns">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="static/images/results_energy.png" alt="Energy per Sample: Classification Task Comparison" style="border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy"/>
            </figure>
          </div>
          <div class="column is-one-third" style="display: flex; align-items: center;">
            <div class="content">
              <p style="font-size: 1rem; line-height: 1.7; color: #4a5568;">
                <strong style="color: #2d3748;">Key Insight:</strong> Generative classification is computationally expensive‚Äîdecoder-only models require <strong>8.7√ó more energy</strong> than BERT to achieve similar accuracy on sentiment analysis tasks.
              </p>
              <p style="font-size: 0.9rem; margin-top: 1rem; padding: 1rem; background: #fef3c7; border-left: 4px solid #f59e0b; border-radius: 6px;">
                üí° <strong>Implication:</strong> Task-specific architectures remain competitive for production NLU workloads.
              </p>
            </div>
          </div>
        </div>
      </div>
      
      <!-- Experiment 2: The Context Stress Test -->
      <div class="box" style="margin-bottom: 3rem; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.08);">
        <h3 class="title is-4" style="color: #2d3748; margin-bottom: 1.5rem;">
          <span style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text;">
            Experiment 2: The Context Stress Test
          </span>
        </h3>
        <div class="columns">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="static/images/results_vram.png" alt="VRAM Growth: Context Length Scaling" style="border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy"/>
            </figure>
          </div>
          <div class="column is-one-third" style="display: flex; align-items: center;">
            <div class="content">
              <p style="font-size: 1rem; line-height: 1.7; color: #4a5568;">
                <strong style="color: #2d3748;">Key Insight:</strong> Decoder-only models hit <strong>OOM at 4K tokens</strong> on V100 GPUs due to quadratic KV-cache growth, while encoder models scale linearly.
              </p>
              <p style="font-size: 0.9rem; margin-top: 1rem; padding: 1rem; background: #fecaca; border-left: 4px solid #ef4444; border-radius: 6px;">
                ‚ö†Ô∏è <strong>Implication:</strong> Long-context applications require architectural innovations (e.g., sparse attention, memory-efficient transformers).
              </p>
            </div>
          </div>
        </div>
      </div>
      
      <!-- Experiment 3: Training Convergence -->
      <div class="box" style="border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.08);">
        <h3 class="title is-4" style="color: #2d3748; margin-bottom: 1.5rem;">
          <span style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text;">
            Experiment 3: Training Convergence
          </span>
        </h3>
        <div class="columns">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="static/images/results_loss_curves.png" alt="Normalized Loss Curves: Training Efficiency" style="border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy"/>
            </figure>
          </div>
          <div class="column is-one-third" style="display: flex; align-items: center;">
            <div class="content">
              <p style="font-size: 1rem; line-height: 1.7; color: #4a5568;">
                <strong style="color: #2d3748;">Key Insight:</strong> Encoder inductive bias leads to <strong>2√ó faster convergence</strong> in domain adaptation scenarios with limited data (<1000 samples).
              </p>
              <p style="font-size: 0.9rem; margin-top: 1rem; padding: 1rem; background: #dbeafe; border-left: 4px solid #3b82f6; border-radius: 6px;">
                üìä <strong>Implication:</strong> Pretraining paradigm choice significantly impacts fine-tuning efficiency in low-resource settings.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Experimental Results -->


<!-- Key Takeaways -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered" style="margin-bottom: 2rem;">Key Takeaways</h2>
      <div class="content" style="font-size: 1.05rem; line-height: 1.8;">
        <div class="box" style="background: linear-gradient(135deg, #fdfbfb 0%, #ebedee 100%); border-radius: 12px; padding: 2.5rem;">
          <div class="columns is-multiline">
            <div class="column is-half">
              <h4 style="color: #2d3748; font-weight: 700; margin-bottom: 1rem;">
                <span style="color: #667eea; font-size: 1.5rem; margin-right: 0.5rem;">üèÜ</span>
                Best for Production NLU
              </h4>
              <p style="color: #4a5568;">
                <strong>Encoder-only (BERT)</strong> models excel in classification, NER, and Q&A with minimal energy overhead.
              </p>
            </div>
            
            <div class="column is-half">
              <h4 style="color: #2d3748; font-weight: 700; margin-bottom: 1rem;">
                <span style="color: #f093fb; font-size: 1.5rem; margin-right: 0.5rem;">üöÄ</span>
                Best for Versatility
              </h4>
              <p style="color: #4a5568;">
                <strong>Decoder-only (GPT-2)</strong> models provide unmatched flexibility for few-shot learning and creative generation.
              </p>
            </div>
            
            <div class="column is-half">
              <h4 style="color: #2d3748; font-weight: 700; margin-bottom: 1rem;">
                <span style="color: #4facfe; font-size: 1.5rem; margin-right: 0.5rem;">‚öñÔ∏è</span>
                Best for Balanced Workloads
              </h4>
              <p style="color: #4a5568;">
                <strong>Encoder-Decoder (T5)</strong> architectures offer a middle ground for translation, summarization, and seq2seq tasks.
              </p>
            </div>
            
            <div class="column is-half">
              <h4 style="color: #2d3748; font-weight: 700; margin-bottom: 1rem;">
                <span style="color: #f59e0b; font-size: 1.5rem; margin-right: 0.5rem;">üåç</span>
                Sustainability Matters
              </h4>
              <p style="color: #4a5568;">
                The "bigger is better" paradigm comes at an environmental cost‚Äî<strong>architecture selection is a sustainability lever</strong>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Key Takeaways -->






<!-- Paper poster - Commented out 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
      </div>
    </div>
  </section>
-->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{E3MiniBenchmark2025,
  title={E¬≥-Mini Benchmark: A Full-Dimensional Evaluation of Efficiency, Energy, and Effectiveness in Transformer Architectures},
  author={Your Name and Mentor Name and Collaborator Name},
  journal={Machine Learning Research},
  year={2025},
  url={https://your-domain.com/e3-benchmark},
  note={An empirical study comparing Encoder-only, Decoder-only, and Encoder-Decoder architectures across three critical dimensions}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
